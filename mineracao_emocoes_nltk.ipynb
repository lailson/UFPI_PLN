{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./env/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in ./env/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./env/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./env/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.12/site-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 21:03:20.902 python[1342:12359] +[IMKClient subclass]: chose IMKClient_Legacy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = [('eu sou admirada por muitos','alegria'),\n",
    "        ('me sinto completamente amado','alegria'),\n",
    "        ('amar e maravilhoso','alegria'),\n",
    "        ('estou me sentindo muito animado novamente','alegria'),\n",
    "        ('eu estou muito bem hoje','alegria'),\n",
    "        ('que belo dia para dirigir um carro novo','alegria'),\n",
    "        ('o dia est· muito bonito','alegria'),\n",
    "        ('estou contente com o resultado do teste que fiz no dia de ontem','alegria'),\n",
    "        ('o amor e lindo','alegria'),\n",
    "        ('nossa amizade e amor vai durar para sempre', 'alegria'),\n",
    "        ('estou amedrontado', 'medo'),\n",
    "        ('ele esta me ameacando a dias', 'medo'),\n",
    "        ('isso me deixa apavorada', 'medo'),\n",
    "        ('este lugar e apavorante', 'medo'),\n",
    "        ('se perdermos outro jogo seremos eliminados e isso me deixa com pavor', 'medo'),\n",
    "        ('tome cuidado com o lobisomem', 'medo'),\n",
    "        ('se eles descobrirem estamos encrencados', 'medo'),\n",
    "        ('estou tremendo de medo', 'medo'),\n",
    "        ('eu tenho muito medo dele', 'medo'),\n",
    "        ('estou com medo do resultado dos meus testes', 'medo')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remoção das stop word \n",
    "# palavras que não tem um significado, não tem relevância para o sentido\n",
    "# é interessante retirar pois pode influenciar negativamente o algoritmo \n",
    "# Ajuda a diminuir a dimensionalidade dos dados tambem\n",
    "# Vamos criar uma lista de stopwords\n",
    "\n",
    "stopwords = ['a', 'agora', 'algum', 'alguma', 'aquele', 'aqueles', 'de', 'deu', 'do', 'e', 'estou', 'esta', 'esta', 'ir', 'meu', 'muito', 'mesmo', 'no', 'nossa', 'o', 'outro', 'para', 'que', 'sem', 'talvez', 'tem', 'tendo','tenha', 'teve', 'tive', 'todo', 'um', 'uma', 'umas', 'uns', 'vou']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsnltk = nltk.corpus.stopwords.words('portuguese')\n",
    "#stopwordsnltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texto):\n",
    "    frases = []\n",
    "    for (palavras, emocao) in texto:\n",
    "        semstop = [p for p in palavras.split() if p.lower() not in stopwordsnltk]\n",
    "        frases.append((' '.join(semstop), emocao))\n",
    "    return frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('admirada muitos', 'alegria'),\n",
       " ('sinto completamente amado', 'alegria'),\n",
       " ('amar maravilhoso', 'alegria'),\n",
       " ('sentindo animado novamente', 'alegria'),\n",
       " ('bem hoje', 'alegria'),\n",
       " ('belo dia dirigir carro novo', 'alegria'),\n",
       " ('dia est· bonito', 'alegria'),\n",
       " ('contente resultado teste fiz dia ontem', 'alegria'),\n",
       " ('amor lindo', 'alegria'),\n",
       " ('amizade amor vai durar sempre', 'alegria'),\n",
       " ('amedrontado', 'medo'),\n",
       " ('ameacando dias', 'medo'),\n",
       " ('deixa apavorada', 'medo'),\n",
       " ('lugar apavorante', 'medo'),\n",
       " ('perdermos outro jogo eliminados deixa pavor', 'medo'),\n",
       " ('tome cuidado lobisomem', 'medo'),\n",
       " ('descobrirem encrencados', 'medo'),\n",
       " ('tremendo medo', 'medo'),\n",
       " ('medo', 'medo'),\n",
       " ('medo resultado testes', 'medo')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_sem_stopword = remove_stopwords(base)\n",
    "base_sem_stopword\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração do radical das palavras (Stemming)\n",
    "# Também server para diminuir a dimencionalidade dos dados\n",
    "# Exemplo\n",
    "# Livr - o\n",
    "# Livr - inho\n",
    "# Livr - eiro\n",
    "# Livr - eco\n",
    "# Note que o radical é Livr \n",
    "# Outro exemplo \n",
    "# inter   - nacion  - al \n",
    "# prefixo - radical - sufixo\n",
    "# perceba que ao extrair apenas o radical das palavras teremos um redução da dimencionalidade\n",
    "\n",
    "def aplicastemmer(texto):\n",
    "    stemmer = nltk.stem.RSLPStemmer() #Português \n",
    "    frasessteming = []\n",
    "    for (palavras, emocao ) in texto:\n",
    "        comsteming = [str(stemmer.stem(p)) for p in palavras.split()  if p not in stopwordsnltk ]\n",
    "        frasessteming.append((comsteming, emocao))\n",
    "    return frasessteming\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['admir', 'muit'], 'alegria'),\n",
       " (['sint', 'complet', 'am'], 'alegria'),\n",
       " (['am', 'maravilh'], 'alegria'),\n",
       " (['sent', 'anim', 'nov'], 'alegria'),\n",
       " (['bem', 'hoj'], 'alegria'),\n",
       " (['bel', 'dia', 'dirig', 'carr', 'nov'], 'alegria'),\n",
       " (['dia', 'est·', 'bonit'], 'alegria'),\n",
       " (['cont', 'result', 'test', 'fiz', 'dia', 'ont'], 'alegria'),\n",
       " (['am', 'lind'], 'alegria'),\n",
       " (['amizad', 'am', 'vai', 'dur', 'sempr'], 'alegria'),\n",
       " (['amedront'], 'medo'),\n",
       " (['ameac', 'dia'], 'medo'),\n",
       " (['deix', 'apavor'], 'medo'),\n",
       " (['lug', 'apavor'], 'medo'),\n",
       " (['perd', 'outr', 'jog', 'elimin', 'deix', 'pav'], 'medo'),\n",
       " (['tom', 'cuid', 'lobisom'], 'medo'),\n",
       " (['descobr', 'encrenc'], 'medo'),\n",
       " (['trem', 'med'], 'medo'),\n",
       " (['med'], 'medo'),\n",
       " (['med', 'result', 'test'], 'medo')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frasescomsteming = aplicastemmer(base)\n",
    "frasescomsteming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos ver que nossa base agora tem apenas o radical, palavras como amado ou amar agora tem o mesmo radical : am\n",
    "# outro exemplo seria o novamente e novo, as duas palavras tem o mesmo radical : nov. Nesse caso são duas palavras com sentidos distintos com o mesmo radical. Esse é aspecto negativo dessa técnica \n",
    "# Depois de aplicar essas duas técnicas agora iremos fazer a listagem de todas as palavras\n",
    "\n",
    "def busca_palavras(frases):\n",
    "    todas_as_palavras = []\n",
    "    for (palavras, emocao) in frases: \n",
    "        todas_as_palavras.extend(palavras)\n",
    "    return todas_as_palavras\n",
    "\n",
    "palavras = busca_palavras(frasescomsteming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admir', 'muit', 'sint', 'complet', 'am', 'am', 'maravilh', 'sent', 'anim', 'nov', 'bem', 'hoj', 'bel', 'dia', 'dirig', 'carr', 'nov', 'dia', 'est·', 'bonit', 'cont', 'result', 'test', 'fiz', 'dia', 'ont', 'am', 'lind', 'amizad', 'am', 'vai', 'dur', 'sempr', 'amedront', 'ameac', 'dia', 'deix', 'apavor', 'lug', 'apavor', 'perd', 'outr', 'jog', 'elimin', 'deix', 'pav', 'tom', 'cuid', 'lobisom', 'descobr', 'encrenc', 'trem', 'med', 'med', 'med', 'result', 'test']\n"
     ]
    }
   ],
   "source": [
    "print(palavras)\n",
    "# Agora temos todos os radicais das palavras da nossa base mas podemos perceber que existem radicaias repetidos etnão ainda falta\n",
    "# mostrar apenas uma vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('am', 4),\n",
       " ('dia', 4),\n",
       " ('med', 3),\n",
       " ('nov', 2),\n",
       " ('result', 2),\n",
       " ('test', 2),\n",
       " ('deix', 2),\n",
       " ('apavor', 2),\n",
       " ('admir', 1),\n",
       " ('muit', 1),\n",
       " ('sint', 1),\n",
       " ('complet', 1),\n",
       " ('maravilh', 1),\n",
       " ('sent', 1),\n",
       " ('anim', 1),\n",
       " ('bem', 1),\n",
       " ('hoj', 1),\n",
       " ('bel', 1),\n",
       " ('dirig', 1),\n",
       " ('carr', 1),\n",
       " ('est·', 1),\n",
       " ('bonit', 1),\n",
       " ('cont', 1),\n",
       " ('fiz', 1),\n",
       " ('ont', 1),\n",
       " ('lind', 1),\n",
       " ('amizad', 1),\n",
       " ('vai', 1),\n",
       " ('dur', 1),\n",
       " ('sempr', 1),\n",
       " ('amedront', 1),\n",
       " ('ameac', 1),\n",
       " ('lug', 1),\n",
       " ('perd', 1),\n",
       " ('outr', 1),\n",
       " ('jog', 1),\n",
       " ('elimin', 1),\n",
       " ('pav', 1),\n",
       " ('tom', 1),\n",
       " ('cuid', 1),\n",
       " ('lobisom', 1),\n",
       " ('descobr', 1),\n",
       " ('encrenc', 1),\n",
       " ('trem', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extração das palavras (radicais) livre que existem na base\n",
    "def busca_frequencia(palavras):\n",
    "    palavras = nltk.FreqDist(palavras)\n",
    "    return palavras\n",
    "\n",
    "frequencia = busca_frequencia(palavras)\n",
    "frequencia.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['admir', 'muit', 'sint', 'complet', 'am', 'maravilh', 'sent', 'anim', 'nov', 'bem', 'hoj', 'bel', 'dia', 'dirig', 'carr', 'est·', 'bonit', 'cont', 'result', 'test', 'fiz', 'ont', 'lind', 'amizad', 'vai', 'dur', 'sempr', 'amedront', 'ameac', 'deix', 'apavor', 'lug', 'perd', 'outr', 'jog', 'elimin', 'pav', 'tom', 'cuid', 'lobisom', 'descobr', 'encrenc', 'trem', 'med'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def busca_palavras_unicas(frequencia):\n",
    "    freq = frequencia.keys()\n",
    "    return freq\n",
    "\n",
    "palavras_unicas = busca_palavras_unicas(frequencia)\n",
    "palavras_unicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'admir': False,\n",
       " 'muit': False,\n",
       " 'sint': False,\n",
       " 'complet': False,\n",
       " 'am': True,\n",
       " 'maravilh': False,\n",
       " 'sent': False,\n",
       " 'anim': False,\n",
       " 'nov': True,\n",
       " 'bem': False,\n",
       " 'hoj': False,\n",
       " 'bel': False,\n",
       " 'dia': True,\n",
       " 'dirig': False,\n",
       " 'carr': False,\n",
       " 'est·': False,\n",
       " 'bonit': False,\n",
       " 'cont': False,\n",
       " 'result': False,\n",
       " 'test': False,\n",
       " 'fiz': False,\n",
       " 'ont': False,\n",
       " 'lind': False,\n",
       " 'amizad': False,\n",
       " 'vai': False,\n",
       " 'dur': False,\n",
       " 'sempr': False,\n",
       " 'amedront': False,\n",
       " 'ameac': False,\n",
       " 'deix': False,\n",
       " 'apavor': False,\n",
       " 'lug': False,\n",
       " 'perd': False,\n",
       " 'outr': False,\n",
       " 'jog': False,\n",
       " 'elimin': False,\n",
       " 'pav': False,\n",
       " 'tom': False,\n",
       " 'cuid': False,\n",
       " 'lobisom': False,\n",
       " 'descobr': False,\n",
       " 'encrenc': False,\n",
       " 'trem': False,\n",
       " 'med': False}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extrator_palavras(documento):\n",
    "    doc = set(documento)\n",
    "    caracteristicas = {}\n",
    "    for palavras in palavras_unicas:\n",
    "        caracteristicas['%s' % palavras] = (palavras in doc)\n",
    "    return caracteristicas\n",
    "\n",
    "caracteristicas_frase = extrator_palavras(['am', 'nov', 'dia'])\n",
    "caracteristicas_frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'admir': True, 'muit': True, 'sint': False, 'complet': False, 'am': False, 'maravilh': False, 'sent': False, 'anim': False, 'nov': False, 'bem': False, 'hoj': False, 'bel': False, 'dia': False, 'dirig': False, 'carr': False, 'est·': False, 'bonit': False, 'cont': False, 'result': False, 'test': False, 'fiz': False, 'ont': False, 'lind': False, 'amizad': False, 'vai': False, 'dur': False, 'sempr': False, 'amedront': False, 'ameac': False, 'deix': False, 'apavor': False, 'lug': False, 'perd': False, 'outr': False, 'jog': False, 'elimin': False, 'pav': False, 'tom': False, 'cuid': False, 'lobisom': False, 'descobr': False, 'encrenc': False, 'trem': False, 'med': False}, 'alegria'), ({'admir': False, 'muit': False, 'sint': True, 'complet': True, 'am': True, 'maravilh': False, 'sent': False, 'anim': False, 'nov': False, 'bem': False, 'hoj': False, 'bel': False, 'dia': False, 'dirig': False, 'carr': False, 'est·': False, 'bonit': False, 'cont': False, 'result': False, 'test': False, 'fiz': False, 'ont': False, 'lind': False, 'amizad': False, 'vai': False, 'dur': False, 'sempr': False, 'amedront': False, 'ameac': False, 'deix': False, 'apavor': False, 'lug': False, 'perd': False, 'outr': False, 'jog': False, 'elimin': False, 'pav': False, 'tom': False, 'cuid': False, 'lobisom': False, 'descobr': False, 'encrenc': False, 'trem': False, 'med': False}, 'alegria'), ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extração das palavras de todas as frases\n",
    "# o aplly  pega cada uma das frases (frasescomsteming) e vai aplicar a função extrator_palavras e o resultado salva em base_completa\n",
    "base_completa = nltk.classify.apply_features(extrator_palavras, frasescomsteming)\n",
    "base_completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'admir': False, 'muit': False, 'sint': False, 'complet': False, 'am': False, 'maravilh': False, 'sent': False, 'anim': False, 'nov': False, 'bem': False, 'hoj': False, 'bel': False, 'dia': False, 'dirig': False, 'carr': False, 'est·': False, 'bonit': False, 'cont': False, 'result': False, 'test': False, 'fiz': False, 'ont': False, 'lind': False, 'amizad': False, 'vai': False, 'dur': False, 'sempr': False, 'amedront': False, 'ameac': False, 'deix': False, 'apavor': False, 'lug': False, 'perd': False, 'outr': False, 'jog': False, 'elimin': False, 'pav': False, 'tom': True, 'cuid': True, 'lobisom': True, 'descobr': False, 'encrenc': False, 'trem': False, 'med': False}, 'medo')\n"
     ]
    }
   ],
   "source": [
    "print(base_completa[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tabela para classificação de sentimentos](./img/tabela_classificcao_sentimento.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "# Muito utiliado para tarefas de mineração de texto \n",
    "# Detecção de emoção com texto usando Naive Bayes\n",
    "# Abordagem totalmente probabilística ( Teorema de Bayes )\n",
    "# Exemplos do uso: Filtros de spam (sim ou não), mineração de emoções, separação de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Base de risco de crédito](./img/tabela_risco_credito.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O algoritmo Naive Bayes vai ler a tabela (base de dados) e vai construir uma tabela de probabilidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tabela de probabilidade do Naive Bayes](./img/naive_bayes_probabilidade.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entendendo um pouco a criação da tabela acima. Primeiramente vão preencher essa parte em azul, colocando quantas vezes o risco de crédito alto parece, o risco de crédito moderado aparece e o risco de crédito baixo aparece, vamos completar a informação com o total de casos com os 3 que totaliza 14, assim 6/14 quer dizer que o risco alto aparece 6 vezes no total de 14 registros, o risco moderado aparece apenas 3 no total de 14 registros e o risco baixa aparece 5 no total de 14 registros. \n",
    "\n",
    "Devemos fazer o mesmo para as features, O histórico de crédito bom aparece 5 vezes, o histórico de crédito desconhecido aparece 5x e o histórico de crédito ruim aparece 4. Agora vamos fazer a relação entre risco de crédito Alto e histórico de crédito bom. Dos 5 casos de histórico de crédito bom 1 tem o risco de crédito alto, dos 5 casos de histórico desconhecido 2 tem o risco de crédito alto, e dos 4 histórico de crédito ruim 3 tem o risco alto. Assim devemos preencher o restante da tabela. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo a previsão do novo registro \n",
    "# Imagine que chegou um novo cliente com histórico de crédito bom, a dívida é alta, a garantia é nenhuma e a renda é maior do que 35mil \n",
    "# Agora, como o Naive Bayes classifica esse cliente ?\n",
    "# Assim, devemos olhar para a tabela anterior e focar nas 4 colunas correspondente à feature do cliente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, como o Naive Bayes classifica esse cliente ?\n",
    "Assim, devemos olhar para a tabela anterior e focar nas 4 colunas correspondente à feature do cliente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cliente risco de crédito](./img/cliente_risco_credito.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caracterísiticas: Boa, Altam Nenhuma, > 35000\n",
    "\n",
    "P(Alto) = (6/14) * (1/5) * (4/7) * (6/11) * (1/7) = 0.003816591571693612\n",
    "\n",
    "P(Moderado) = (3/14) * (1/5) * (1/7) * (2/11) * (1/7) = 0.0001590246488205672\n",
    "\n",
    "P(Baixo) = (5/14) * (3/5) * (2/7) * (3/11) * (5/7) = 0.011926848661542536\n",
    "\n",
    "O maior resultado entre os 3 é o 0.0119 , sendo assim, o Naive Bayes vai classificar como risco Baixo \n",
    "\n",
    "Vamos somar a probabilidade dos 3 casos: 0.0038 + 0.0001 + 0.0119 = 0.0158 (100%)\n",
    "\n",
    "Agora vamos ver a probabilidade de cada caso:\n",
    "\n",
    "P(Alto) = 0.0038 / 0.0158 = 24,05%\n",
    "\n",
    "P(Moderado) = 0.001 / 0.0158 = 0,63%\n",
    "\n",
    "P(Baixo) = 0.0119 / 0.0158 = 75,31%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes em textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Base de dados textual para classificação de emoções](./img/alegriaxmedo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tabela de dados geral](./img/tabela_texto_alegria_medo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando a tabela de probabilidades\n",
    "\n",
    "![Tabela de probabilidade do texto com Naive Bayes](./img/tabela_probabilidade_texto_naive_bayes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma explicação rápida da tabela de probabilidades:\n",
    "- Temos 3 registros de alegria de um total de 5\n",
    "- Temos 2 registros de medo de um total de 5\n",
    "- O radical 'sint' aparece em duas frases \n",
    "- O radical 'sint' não aparece em três frases\n",
    "- A probabilidade do 'sint' = 's' e a classificação é 'alegria' = 2/2\n",
    "- A probabilidade do 'sint' = 'n'e a classificação é 'medo' = 0\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imaginando que vamos receber a seguinte frase: \"Me sinto completamente apavorado por este lugar\"\n",
    "\n",
    "Vamos utilizar a tabela para saber a clasiificação \n",
    "\n",
    "Primeiramente devomos fazer a remoç˜ão das Stopwords: \"sinto completamente apavorado lugar\"\n",
    "\n",
    "Agora vamos extrair o radical dessas palavras: \"sint complet pavor lug\"\n",
    "\n",
    "Assim podemos perceber que os radicais [sint, complet, pavor, lug] terão valor S pois estão presente na frase\n",
    "\n",
    "Temos que mostrar agora que o restante dos radiciais não aparecem na frase, sendo assim [am, bem, hoj, deix] são os radicais que não estão no texto por isso devemos informar que esses radicais tem valor N.\n",
    "\n",
    "sint = S, complet=S, am=N, bem=N, hoje=N, apavor=S, lug=S\n",
    "\n",
    "![Tabela com a frase: Me sinto complemetante apavorado por este lugar](./img/tabela_mesintocompletamenteapavoradoporestelugar.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(Alegria) = probabilidade_de_ser_alegria * probabilidade_de_sint=S_alegria * probabilidade_complet=S_alegria * probabilidade_am=N_alegria * probabilidade_bem=N_alegria * probabilidade_hoje=N_alegria * probabilidade_deixa=N_alegria * probabilidade_apavor=S_alegria * probabilidade_lug=S_alegria\n",
    "\n",
    "P(Medo) = probabilidade_de_ser_medo * probabilidade_sint=S_medo * probabilidade_complet=S_medo * probabilidade_am=N_medo * probabilidade_bem=N_medo * probabilidade_deix=N_medo * probabilidade_apavor=S_medo * probabilidade_lug=S_medo\n",
    "\n",
    "----\n",
    "\n",
    "P(Alegria) = 3/5 * 2/2 * 1/1 * 1/3 * 2/4 * 2/4 * 3/4 * 0 * 0 = \n",
    "\n",
    "P(Medo) = 2/5 * 0 * 0 * 2/3 * 2/4 * 2/4 * 1/4 * 2/2 * 1/1 = \n",
    "\n",
    "Podemos perceber que temos multiplicação por 0, assim nessa probabilidade fica fácil perceber que as duas probabilidade seriam 0.\n",
    "\n",
    "O algorimto Naive Bayes faz a correção Lapaciana que evita a multiplicação por 0. Assim, quando o algoritmo encontra uma frase que não tem nenhum registro então automaticamente adiciona um registro, para que não aconteça a multiplicação por zero. Ao adiconar um registro perceba que a porcentagem dos demais é alterada, em 'sint'=S e medo não tem registro, então é adicinado 1 então o percentual de medo que antes era 2/5 passa a ser 2/6 e alegria que antes tinha 3/5 passa a ser 3/6. Os registros de sint=S passa de 2 para 3 e por fim, sint=S em alegria passa de 2/2 para 2/3. Isso tudo porque foi adicionado um registro sint=S em medo, quando for adicionado um registro onde tem probabilidade 0 então deve se recalculado todas as outras probabilidade.\n",
    "\n",
    "Vamos analisar um pouco esse acrécismo que é feito. Se a base de dados é muito pequena como o exemplo acima isso pode ser uma grande probelma, veja que sint=s em alegria tem a probabilidade 2/3 (depois do acréscimo) e o registro que foi adicionado gerou uma probabilidade 1/3, em termos comparativos 2/3 é muito próximo de 1/3 e isso é um problema. Quando a base é bem maior esse acréscimo é imperceptível e não gera problema. \n",
    "\n",
    "Para exemplos didáticos vou apenas desconsiderar o zero e não vou fazer o acréscimo do registro. \n",
    "\n",
    "P(Alegria) = 3/5 * 2/2 * 1/1 * 1/3 * 2/4 * 2/4 * 3/4  = 0.0375\n",
    "\n",
    "P(Medo) = 2/5 * 2/3 * 2/4 * 2/4 * 1/4 * 2/2 * 1/1 = 0.0167\n",
    "\n",
    "0.037 é maior do que 0.0167 então essa frase é classificada como Alegria. Vamos apenas calcular a porcentagem:\n",
    "\n",
    "(0.0375 + 0.0167) = 0.0541 (100%)\n",
    "\n",
    "P(Alegria) = 0.0375 / 0.0541 * 100 = 69.31%\n",
    "\n",
    "P(Medo) = 0.0375 / 0.0541 * 100 = 30.69%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
